{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excited-device",
   "metadata": {},
   "source": [
    "# Example of gridded forecasts postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-election",
   "metadata": {},
   "source": [
    "In this notebook, using Pythie, we postprocess the 2 metre temperature forecasts at a station. We postprocess it with the 2 metre temperature itself, the maximum 2 metre temperature in the last 6 hours and the soil temperature as predictors.\n",
    "\n",
    "We use the [ERA5 reanalysis](https://climate.copernicus.eu/climate-reanalysis) over a large area in Europe from 1997 to 2016 as gridded observations. These reanalysis have been downloaded from the [Copernicus Data Store](https://cds.climate.copernicus.eu) and converted to the [NetCDF](https://en.wikipedia.org/wiki/NetCDF) file format.\n",
    "\n",
    "This notebook uses the [iris](https://github.com/SciTools/iris) package to load and plot the NetCDF files.\n",
    "\n",
    "The postprocessing is done by making a regression, at each grid point and lead time, between the gridded reforecasts and the gridded reanalysis (considered as observation). For verification, the result of this regression is then applied on the reforecasts themselves (the training set).\n",
    "\n",
    "The reforecasts files have been download from [ECMWF](https://www.ecmwf.int/) and converted to NetCDF files.\n",
    "\n",
    "**Note:** *In the following example, we drop the initial conditions of the reforecasts because one of the maximum 2 meter temperature is not defined at this lead time ! As a result, we do not postprocess the lead time 0.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-society",
   "metadata": {},
   "source": [
    "**Warning:** To perform the computation of this example notebook, you need first to download from [Zenodo](https://zenodo.org/) the gridded observation and reforecast dataset. You also need to install the extra packages. See the [README.md](../README.md) file for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-hello",
   "metadata": {},
   "source": [
    "#### Gridded reforecast data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-housing",
   "metadata": {},
   "source": [
    "Source: www.ecmwf.int\n",
    "\n",
    "Creative Commons Attribution 4.0 International (CC BY 4.0)\n",
    "Copyright © 2021 European Centre for Medium-Range Weather Forecasts (ECMWF).\n",
    "See the attached ECMWF_LICENSE.txt file included with the data for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-massage",
   "metadata": {},
   "source": [
    "#### Copernicus ERA5 gridded reanalysis data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-mitchell",
   "metadata": {},
   "source": [
    "Source: [https://cds.climate.copernicus.eu/](https://cds.climate.copernicus.eu/)\n",
    "\n",
    "Copyright © 2021 European Union.\n",
    "Generated using Copernicus Climate Change Service information 2021.\n",
    "\n",
    "Hersbach et al. (2018): ERA5 hourly data on single levels from 1979 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS). (Accessed on < 21-04-2021 >), [doi:10.24381/cds.adbb2d47](https://doi.org/10.24381/cds.adbb2d47).\n",
    "\n",
    "See the attached COPERNICUS_LICENSE.txt file included with the data for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-career",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-twenty",
   "metadata": {},
   "source": [
    "Setting the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.extend([os.path.abspath('../')])\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-montreal",
   "metadata": {},
   "source": [
    "Importing external modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import iris.pandas as ipd\n",
    "import iris.plot as iplt\n",
    "import iris.quickplot as qplt\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import datetime\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','sans-serif':['Times'],'size':16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-dimension",
   "metadata": {},
   "source": [
    "Importing internal modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data import Data\n",
    "import postprocessors.MBM as MBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-samuel",
   "metadata": {},
   "source": [
    "Setting some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date of the forecast\n",
    "date = \"01-02\"\n",
    "# Year to correct\n",
    "year = 2017\n",
    "# Number of reforecasts\n",
    "years_back = 20\n",
    "# Parameter of the analysis (to be postprocessed)\n",
    "param = '2t'\n",
    "# Parameters of the predictors\n",
    "params = ['2t', 'mx2t6', 'stl1']\n",
    "# Time units used in the NetCDF input files\n",
    "time_units = 'hours since 1900-01-01 00:00:00.0'\n",
    "# Locaction of the data\n",
    "data_folder = './data/europa_grid/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-houston",
   "metadata": {},
   "source": [
    "Defining some functions to extract the time coordinates from Iris cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cubes_time_index(cubes):\n",
    "    # assumes all the cube in cubes have the same timestamps\n",
    "    a = ipd.as_data_frame(cubes[0][:, 0, 0])\n",
    "    return a.index\n",
    "\n",
    "\n",
    "def get_cubes_time(cubes, units=None, year=None):\n",
    "    time_index = get_cubes_time_index(cubes)\n",
    "    if year is not None:\n",
    "        convert_time = lambda t: datetime.datetime(year, t.month, t.day, t.hour)\n",
    "    else:\n",
    "        convert_time = lambda t: datetime.datetime(t.year, t.month, t.day, t.hour)\n",
    "    time = np.array(list(map(convert_time, time_index.values)), dtype=object)\n",
    "    if units is not None:\n",
    "        cf_time = cftime.date2num(time, units=units)\n",
    "        return time, cf_time\n",
    "    else:\n",
    "        return time, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-hunger",
   "metadata": {},
   "source": [
    "Defining some plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cube(cube, t, v, ax=None, quick=True):\n",
    "    if ax is None:\n",
    "        map_proj = ccrs.PlateCarree()\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        ax = plt.axes(projection=map_proj)\n",
    "    extent = [-20, 20, 30, 70]\n",
    "    ax.gridlines()\n",
    "    ax.coastlines(resolution='50m')\n",
    "    ax.add_feature(cartopy.feature.BORDERS)\n",
    "    if quick:\n",
    "        qplt.contourf(cube[t, :, :], v, axes=ax, cmap=cm.get_cmap('bwr'))\n",
    "    else:\n",
    "        iplt.contourf(cube[t, :, :], v, axes=ax, cmap=cm.get_cmap('bwr'))\n",
    "    ax.set_extent(extent)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_quant(quant, labels, timestamps, grid_point):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    first = True\n",
    "    for c, lab in zip(quant, labels):\n",
    "        if first:\n",
    "            c.plot(ax=ax, global_label=lab, timestamps=timestamps, lw=3., ls=\"--\", grid_point=grid_point)\n",
    "            first = False\n",
    "        else:\n",
    "            c.plot(ax=ax, global_label=lab, timestamps=timestamps, grid_point=grid_point)\n",
    "\n",
    "    ax.legend()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-timing",
   "metadata": {},
   "source": [
    "## Loading and creating the Data objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-thomas",
   "metadata": {},
   "source": [
    "**This section shows how to load Data objects from NetCDF files using Iris**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-sunday",
   "metadata": {},
   "source": [
    "First we load the reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each year in the past:\n",
    "for y in range(year-years_back, year):\n",
    "    # We load the reforecasts with Iris\n",
    "    reforecast_cubes = iris.load(data_folder+'reforecasts_'+'-'.join(params)+'_'+str(y)+'-'+date+'.nc')\n",
    "    # If it is the first year, then:\n",
    "    if y == year-years_back:\n",
    "        # We create one list per predictors:\n",
    "        reforecasts_list = list()\n",
    "        for i in range(len(reforecast_cubes)):\n",
    "            reforecasts_list.append(list())\n",
    "        # And a list for the timestamps:\n",
    "        times = list()\n",
    "    # We extract the time:\n",
    "    time, t = get_cubes_time(reforecast_cubes, time_units)\n",
    "    times.append(time[1:, ...])\n",
    "    # We create a Numpy array for each variable from the Iris cube \n",
    "    for i, cube in enumerate(reforecast_cubes):\n",
    "        # (we have to swap the time and ensemble member axis and add 2 dummy axis at the beginning \n",
    "        # to fit the Data object specs)\n",
    "        data = np.array(cube.data[1:, ...]).swapaxes(0, 1)[np.newaxis, np.newaxis, ...]\n",
    "        # (and also add the dummy 'variable' axis)\n",
    "        data = np.expand_dims(data, 3)\n",
    "        reforecasts_list[i].append(data)\n",
    "\n",
    "# Now that the data have been loaded into convenient Numpy arrays, we can create the Data object:\n",
    "# First create one Data object per predictor\n",
    "reforecasts_data_list = list()\n",
    "for reforecast in reforecasts_list:\n",
    "    reforecasts_data_list.append(Data(np.concatenate(reforecast, axis=1), timestamps=times))\n",
    "# saving the first predictor (the variable itself) for latter\n",
    "reforecast_data_1st_predictor = reforecasts_data_list[0].copy()\n",
    "# (Deleting data not needed anymore to save RAM space)\n",
    "del reforecast_cubes, reforecasts_list\n",
    "# Then loading all the predictors into one single Data object\n",
    "reforecasts_data = reforecasts_data_list[0].copy()\n",
    "for reforecast in reforecasts_data_list[1:]:\n",
    "    reforecasts_data.append_predictors(reforecast)\n",
    "# (Deleting data not needed anymore to save RAM space)\n",
    "del reforecasts_data_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-guatemala",
   "metadata": {},
   "source": [
    "Then we load the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as previously but fo only one variable\n",
    "analysis = list()\n",
    "times = list()\n",
    "for y in range(year-years_back, year):\n",
    "    analysis_cube = iris.load(data_folder+'analysis_'+param+'_'+str(y)+'-'+date+'.nc')\n",
    "    time, t = get_cubes_time(analysis_cube, time_units)\n",
    "    times.append(time[1:, ...])\n",
    "    data = np.array(analysis_cube[0].data[1:, ...])[np.newaxis, np.newaxis, np.newaxis, np.newaxis, ...]\n",
    "    analysis.append(data)\n",
    "\n",
    "analysis_data = Data(np.concatenate(analysis, axis=1), timestamps=times)\n",
    "\n",
    "del analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-champagne",
   "metadata": {},
   "source": [
    "## Training the PostProcessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-milan",
   "metadata": {},
   "source": [
    "In this section, we train the various different postprocessors of the Member-By-Member MBM module with the data previously loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold the trained PostProcessors\n",
    "postprocessors = list()\n",
    "proc_labels = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-nickel",
   "metadata": {},
   "source": [
    "### Simple bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ebc = MBM.BiasCorrection()\n",
    "ebc.train(analysis_data, reforecasts_data)\n",
    "postprocessors.append(ebc)\n",
    "proc_labels.append('Bias correction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-atmosphere",
   "metadata": {},
   "source": [
    "### Ensemble Mean correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "emc = MBM.EnsembleMeanCorrection()\n",
    "emc.train(analysis_data, reforecasts_data)\n",
    "postprocessors.append(emc)\n",
    "proc_labels.append('Ensemble Mean correction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-motel",
   "metadata": {},
   "source": [
    "### Ensemble Spread Scaling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "essc = MBM.EnsembleSpreadScalingCorrection()\n",
    "essc.train(analysis_data, reforecasts_data)\n",
    "postprocessors.append(essc)\n",
    "proc_labels.append('Ensemble Spread Scaling correction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-doctrine",
   "metadata": {},
   "source": [
    "### Ensemble Spread Scaling correction with Absolute norm CRPS minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "essacc = MBM.EnsembleSpreadScalingAbsCRPSCorrection()\n",
    "essacc.train(analysis_data, reforecasts_data, ntrial=1)\n",
    "postprocessors.append(essacc)\n",
    "proc_labels.append('Ensemble Spread Scaling Abs. CRPS min. correction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-equipment",
   "metadata": {},
   "source": [
    "### Ensemble Spread Scaling correction + Climatology nudging with Absolute norm CRPS minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "eacc = MBM.EnsembleAbsCRPSCorrection()\n",
    "eacc.train(analysis_data, reforecasts_data, ntrial=1)\n",
    "postprocessors.append(eacc)\n",
    "proc_labels.append('Ensemble Spread Scaling + Clim. Nudging Abs. CRPS min. correction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-mexican",
   "metadata": {},
   "source": [
    "### Ensemble Spread Scaling correction + Climatology nudging with Ngr CRPS minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "encc = MBM.EnsembleNgrCRPSCorrection()\n",
    "encc.train(analysis_data, reforecasts_data, ntrial=1)\n",
    "postprocessors.append(encc)\n",
    "proc_labels.append('Ensemble Spread Scaling + Clim. Nudging Ngr CRPS min. correction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-graduation",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-robertson",
   "metadata": {},
   "source": [
    "Here we are going to postprocess the reforecasts themselves to see how well they perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the experiment names\n",
    "exp_results = list()\n",
    "exp_results.append(reforecast_data_1st_predictor)\n",
    "exp_labels = list()\n",
    "exp_labels.append('Raw forecasts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.array(range(reforecast_data_1st_predictor.number_of_time_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, postprocessor in zip(proc_labels, postprocessors):\n",
    "    exp_results.append(postprocessor(reforecasts_data))\n",
    "    exp_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-dividend",
   "metadata": {},
   "source": [
    "### Computing scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-buffer",
   "metadata": {},
   "source": [
    "Computing the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the CRPS Data object\n",
    "bias = list()\n",
    "for label, result in zip(exp_labels, exp_results):\n",
    "    bias.append(result.bias(analysis_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-queue",
   "metadata": {},
   "source": [
    "Computing the ensemble mean RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the CRPS Data object\n",
    "rmse = list()\n",
    "for label, result in zip(exp_labels, exp_results):\n",
    "    rmse.append(result.ensemble_mean_RMSE(analysis_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-thousand",
   "metadata": {},
   "source": [
    "Computing the CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the CRPS Data object\n",
    "crps = list()\n",
    "for label, result in zip(exp_labels, exp_results):\n",
    "    crps.append(result.CRPS(analysis_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-instrument",
   "metadata": {},
   "source": [
    "### Plotting the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-desert",
   "metadata": {},
   "source": [
    "#### Plots at a grid point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_point=(20,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_quant(crps, exp_labels, timestamps, grid_point=grid_point)\n",
    "ax.set_title('CRPS Score at grid point '+str(grid_point))\n",
    "ax.set_ylabel('CRPS [C°]')\n",
    "ax.set_xlabel('time x 6hrs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_quant(rmse, exp_labels, timestamps, grid_point=grid_point)\n",
    "ax.set_title('RMSE Score at grid point '+str(grid_point))\n",
    "ax.set_ylabel('RMSE [C°]')\n",
    "ax.set_xlabel('time x 6hrs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_quant(bias, exp_labels, timestamps, grid_point=grid_point)\n",
    "ax.set_title('Bias Score at grid point '+str(grid_point))\n",
    "ax.set_ylabel('Bias [C°]')\n",
    "ax.set_xlabel('time x 6hrs');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-ladder",
   "metadata": {},
   "source": [
    "#### Plotting scores as fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-cooperative",
   "metadata": {},
   "source": [
    "Creating Iris CubeLists to store the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_cubes = iris.cube.CubeList()\n",
    "bias_cubes = iris.cube.CubeList()\n",
    "rmse_cubes = iris.cube.CubeList()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-fruit",
   "metadata": {},
   "source": [
    "Creating the bias cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in bias:\n",
    "    # Creating a cube by recycling the analysis one\n",
    "    bias_cube = analysis_cube[0].copy()\n",
    "    # And replacing the data inside by the score\n",
    "    bias_cube.data[1:, :, :] = np.squeeze(score.data)\n",
    "    bias_cubes.append(bias_cube[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in rmse:\n",
    "    # Creating a cube by recycling the analysis one\n",
    "    rmse_cube = analysis_cube[0].copy()\n",
    "    # And replacing the data inside by the score\n",
    "    rmse_cube.data[1:, :, :] = np.squeeze(score.data)\n",
    "    rmse_cubes.append(rmse_cube[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in crps:\n",
    "    # Creating a cube by recycling the analysis one\n",
    "    crps_cube = analysis_cube[0].copy()\n",
    "    # And replacing the data inside by the score\n",
    "    crps_cube.data[1:, :, :] = np.squeeze(score.data)\n",
    "    crps_cubes.append(crps_cube[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countour plot levels in Celsius\n",
    "t2m_range = np.arange(-4.,4.,0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-joint",
   "metadata": {},
   "source": [
    "Plot of the CRPS improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = crps_cubes[0].shape[0]\n",
    "map_proj = ccrs.PlateCarree()\n",
    "ns = len(rmse_cubes)-1\n",
    "nr = int(np.ceil(ns / 2))\n",
    "axls = list()\n",
    "fig=plt.figure(figsize=(15, 10 * nr))\n",
    "t = 0\n",
    "for i in range(ns):\n",
    "    plt.subplot(nr, 2, i+1, projection=map_proj)\n",
    "    ax = plt.gca()\n",
    "    axls.append(ax)\n",
    "    plot_cube(crps_cubes[i+1]-crps_cubes[0], t, t2m_range, ax=ax)\n",
    "    ax.set_title(exp_labels[i+1]+' method \\nCRPS change w.r.t. the raw forecasts at time '+str((t+1)*6)+' hrs\\n Blue is better, red is worse')\n",
    "    \n",
    "def update(t):\n",
    "    imlist = list()\n",
    "    for i in range(ns):\n",
    "        ax = axls[i]\n",
    "        ax.clear()\n",
    "        plot_cube(crps_cubes[i+1]-crps_cubes[0], t, t2m_range, ax=ax, quick=False)\n",
    "        ax.set_title(exp_labels[i+1]+' method \\nCRPS change w.r.t. the raw forecasts at time '+str((t+1)*6)+' hrs\\n Blue is better, red is worse')\n",
    "        imlist.append(ax._gci())\n",
    "    return imlist\n",
    "            \n",
    "crps_anim = animation.FuncAnimation(fig, update, frames=frames, interval=250, blit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-tyler",
   "metadata": {},
   "source": [
    "Making a video to show the CRPS evolution over the lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-instrument",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(crps_anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-playing",
   "metadata": {},
   "source": [
    "Plot of the RMSE improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = bias_cubes[0].shape[0]\n",
    "map_proj = ccrs.PlateCarree()\n",
    "ns = len(rmse_cubes)-1\n",
    "nr = int(np.ceil(ns / 2))\n",
    "axls = list()\n",
    "fig=plt.figure(figsize=(15, 10 * nr))\n",
    "t = 0\n",
    "for i in range(ns):\n",
    "    plt.subplot(nr, 2, i+1, projection=map_proj)\n",
    "    ax = plt.gca()\n",
    "    axls.append(ax)\n",
    "    plot_cube(rmse_cubes[i+1]-rmse_cubes[0], t, t2m_range, ax=ax)\n",
    "    ax.set_title(exp_labels[i+1]+' method \\nRMSE change w.r.t. the raw forecasts at time '+str((t+1)*6)+' hrs\\n Blue is better, red is worse')\n",
    "    \n",
    "def update(t):\n",
    "    imlist = list()\n",
    "    for i in range(ns):\n",
    "        ax = axls[i]\n",
    "        ax.clear()\n",
    "        plot_cube(rmse_cubes[i+1]-rmse_cubes[0], t, t2m_range, ax=ax, quick=False)\n",
    "        ax.set_title(exp_labels[i+1]+' method \\nRMSE change w.r.t. the raw forecasts at time '+str((t+1)*6)+' hrs\\n Blue is better, red is worse')\n",
    "        imlist.append(ax._gci())\n",
    "    return imlist\n",
    "            \n",
    "rmse_anim = animation.FuncAnimation(fig, update, frames=frames, interval=250, blit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-appendix",
   "metadata": {},
   "source": [
    "Making a video to show the RMSE evolution over the lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-ukraine",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(rmse_anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-series",
   "metadata": {},
   "source": [
    "Plot of the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-methodology",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frames = bias_cubes[0].shape[0]\n",
    "map_proj = ccrs.PlateCarree()\n",
    "ns = len(bias_cubes)\n",
    "nr = int(np.ceil(ns / 2))\n",
    "axls = list()\n",
    "fig=plt.figure(figsize=(15, 10 * nr))\n",
    "t = 0\n",
    "for i in range(ns):\n",
    "    plt.subplot(nr, 2, i+1, projection=map_proj)\n",
    "    ax = plt.gca()\n",
    "    axls.append(ax)\n",
    "    plot_cube(bias_cubes[i], t, t2m_range, ax=ax)\n",
    "    if i >= 1: \n",
    "        ax.set_title(exp_labels[i]+' method \\nBias score at time '+str((t+1)*6)+' hrs')\n",
    "    else:\n",
    "        ax.set_title(exp_labels[i]+'\\nBias score at time '+str((t+1)*6)+' hrs')\n",
    "\n",
    "def update(t):\n",
    "    imlist = list()\n",
    "    for i in range(ns):\n",
    "        ax = axls[i]\n",
    "        ax.clear()\n",
    "        plot_cube(bias_cubes[i], t, t2m_range, ax=ax, quick=False)\n",
    "        if i >= 1: \n",
    "            ax.set_title(exp_labels[i]+' method \\nBias score at time '+str((t+1)*6)+' hrs')\n",
    "        else:\n",
    "            ax.set_title(exp_labels[i]+'\\nBias score at time '+str((t+1)*6)+' hrs')\n",
    "        imlist.append(ax._gci())\n",
    "    return imlist\n",
    "            \n",
    "bias_anim = animation.FuncAnimation(fig, update, frames=frames, interval=250, blit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-coupon",
   "metadata": {},
   "source": [
    "Making a video to show the bias evolution over the lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-participant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(bias_anim.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
